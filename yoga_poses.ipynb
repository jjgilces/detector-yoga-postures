{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSUD3mDAujuj"
      },
      "source": [
        "# Recognition of Yoga Postures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTnV1lGbu1OG"
      },
      "source": [
        "## Necessary libraries\n",
        "We choose\n",
        "[Tensorflow](https://pytorch.org/)\n",
        "as our framework for this project for its learning parh and It is Simple GPU installation.\n",
        "And we are going to work with"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wdFyomexudoh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "import shutil\n",
        "import numpy as np\n",
        "import random\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.models import Sequential\n",
        "from sklearn.metrics import classification_report\n",
        "from shutil import copyfile\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from utils import split_data\n",
        "from fastai.vision.all import get_image_files,verify_images\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "from tensorflow.keras.losses import categorical_crossentropy,SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.optimizers.legacy import Adam\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "import datetime\n",
        "from tensorflow.keras.utils import plot_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU con soporte CUDA disponible\n"
          ]
        }
      ],
      "source": [
        "# check if CUDA is available to use GPU on the training \n",
        "train_on_gpu = tf.config.list_physical_devices('GPU')\n",
        "if train_on_gpu:\n",
        "    print('GPU con soporte CUDA disponible')\n",
        "else:\n",
        "    print('No se encontró GPU con soporte CUDA')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We r going to delet images \"broken\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "images=get_image_files('../YOGA/dataset')\n",
        "images\n",
        "failed = verify_images(images)\n",
        "failed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "images=get_image_files('../YOGA/FINALDATA')\n",
        "images\n",
        "failed = verify_images(images)\n",
        "failed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pezi0oUO0U54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Splitting data completed.\n",
            "Splitting data completed.\n",
            "Splitting data completed.\n",
            "Splitting data completed.\n"
          ]
        }
      ],
      "source": [
        "# Define paths for the images\n",
        "downdog_SRC_DIR = \"FINALDATA/downdog\"\n",
        "plank_SRC_DIR = \"FINALDATA/plank\"\n",
        "tree_SRC_DIR = \"FINALDATA/tree\"\n",
        "warrior_SRC_DIR = \"FINALDATA/warrior\"\n",
        "TRAINING_DIR = \"dataset/training\"\n",
        "VALIDATION_DIR = \"dataset/validation\"\n",
        "TEST_DIR = \"dataset/test\"\n",
        "#splitting the dataset\n",
        "TRAINING_DDG_DIR = os.path.join(TRAINING_DIR, \"downdog/\")\n",
        "VALIDATION_DDG_DIR = os.path.join(VALIDATION_DIR, \"downdog/\")\n",
        "TEST_DDG_DIR = os.path.join(TEST_DIR, \"downdog/\")\n",
        "\n",
        "TRAINING_PLANK_DIR = os.path.join(TRAINING_DIR, \"plank/\")\n",
        "VALIDATION_PLANK_DIR = os.path.join(VALIDATION_DIR, \"plank/\")\n",
        "TEST_PLANK_DIR = os.path.join(TEST_DIR, \"plank/\")\n",
        "\n",
        "TRAINING_TREE_DIR = os.path.join(TRAINING_DIR, \"tree/\")\n",
        "VALIDATION_TREE_DIR = os.path.join(VALIDATION_DIR, \"tree/\")\n",
        "TEST_TREE_DIR = os.path.join(TEST_DIR, \"tree/\")\n",
        "\n",
        "TRAINING_WR_DIR = os.path.join(TRAINING_DIR, \"warrior/\")\n",
        "VALIDATION_WR_DIR = os.path.join(VALIDATION_DIR, \"warrior/\")\n",
        "TEST_WR_DIR = os.path.join(TEST_DIR, \"warrior/\")\n",
        "# Define proportion of images used for training\n",
        "train_size = 0.8\n",
        "val_size = 0.1\n",
        "test_size = 0.1\n",
        "\n",
        "split_data(downdog_SRC_DIR,\n",
        "           TRAINING_DDG_DIR,\n",
        "           VALIDATION_DDG_DIR,\n",
        "           TEST_DDG_DIR,\n",
        "           split_ratio=(train_size,val_size, test_size))\n",
        "\n",
        "split_data(plank_SRC_DIR,\n",
        "           TRAINING_PLANK_DIR,\n",
        "           VALIDATION_PLANK_DIR,\n",
        "           TEST_PLANK_DIR,\n",
        "           split_ratio=(train_size,val_size, test_size))\n",
        "\n",
        "split_data(tree_SRC_DIR,\n",
        "           TRAINING_TREE_DIR,\n",
        "           VALIDATION_TREE_DIR,\n",
        "           TEST_TREE_DIR,\n",
        "           split_ratio=(train_size,val_size, test_size))\n",
        "split_data(warrior_SRC_DIR,\n",
        "           TRAINING_WR_DIR,\n",
        "           VALIDATION_WR_DIR,\n",
        "           TEST_WR_DIR,\n",
        "           split_ratio=(train_size,val_size, test_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Downdog directory has 700 images\n",
            "Original Plank directory has 701 images\n",
            "Original Tree directory has 665 images\n",
            "Original Warrior directory has 700 images\n",
            "\n",
            "There are 560 images of Dogdown for training\n",
            "There are 560 images of Plank for training\n",
            "There are 532 images of Tree for training\n",
            "There are 560 images of Warrior for training\n",
            "\n",
            "There are 70 images of Dogdown for validation\n",
            "There are 70 images of Plank for validation\n",
            "There are 66 images of Tree for validation\n",
            "There are 70 images of Warrior for validation\n",
            "\n",
            "There are 70 images of Dogdown for test\n",
            "There are 71 images of Plank for test\n",
            "There are 67 images of Plank for test\n",
            "There are 70 images of tree for test\n"
          ]
        }
      ],
      "source": [
        "print(f\"Original Downdog directory has {len(os.listdir(downdog_SRC_DIR))} images\")\n",
        "print(f\"Original Plank directory has {len(os.listdir(plank_SRC_DIR))} images\")\n",
        "print(f\"Original Tree directory has {len(os.listdir(tree_SRC_DIR))} images\")\n",
        "print(f\"Original Warrior directory has {len(os.listdir(warrior_SRC_DIR))} images\\n\")\n",
        "\n",
        "# Training and validation splits\n",
        "print(f\"There are {len(os.listdir(TRAINING_DDG_DIR))} images of Dogdown for training\")\n",
        "print(f\"There are {len(os.listdir(TRAINING_PLANK_DIR))} images of Plank for training\")\n",
        "print(f\"There are {len(os.listdir(TRAINING_TREE_DIR))} images of Tree for training\")\n",
        "print(f\"There are {len(os.listdir(TRAINING_WR_DIR))} images of Warrior for training\\n\")\n",
        "\n",
        "print(f\"There are {len(os.listdir(VALIDATION_DDG_DIR))} images of Dogdown for validation\")           \n",
        "print(f\"There are {len(os.listdir(VALIDATION_PLANK_DIR))} images of Plank for validation\")\n",
        "print(f\"There are {len(os.listdir(VALIDATION_TREE_DIR))} images of Tree for validation\")\n",
        "print(f\"There are {len(os.listdir(VALIDATION_WR_DIR))} images of Warrior for validation\\n\")\n",
        "\n",
        "print(f\"There are {len(os.listdir(TEST_DDG_DIR))} images of Dogdown for test\")\n",
        "print(f\"There are {len(os.listdir(TEST_PLANK_DIR))} images of Plank for test\")\n",
        "print(f\"There are {len(os.listdir(TEST_TREE_DIR))} images of Plank for test\")\n",
        "print(f\"There are {len(os.listdir(TEST_WR_DIR))} images of tree for test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_euw7YFhx58o"
      },
      "source": [
        "## Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "qOq3rJ-l0Okp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 2209 images belonging to 4 classes.\n",
            "Found 276 images belonging to 4 classes.\n",
            "Found 277 images belonging to 4 classes.\n"
          ]
        }
      ],
      "source": [
        "# Definir un generador de imágenes con transformaciones\n",
        "batch_size = 32\n",
        "width=224\n",
        "height=224\n",
        "posture_names = [\"downdog\", \"plank\", \"tree\", \"warrior\"]\n",
        "\n",
        "# Generador con data augmentation para entrenamiento\n",
        "train_data_gen = ImageDataGenerator(\n",
        "    rescale=1.0/255,\n",
        "    rotation_range=10,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest',\n",
        "    brightness_range=[0.6,1.0]\n",
        ")\n",
        "\n",
        "# Generador sin data augmentation para validación y prueba\n",
        "val_test_data_gen = ImageDataGenerator(\n",
        "    rescale=1.0/255\n",
        ")\n",
        "\n",
        "train_data = train_data_gen.flow_from_directory(\n",
        "    TRAINING_DIR,\n",
        "    classes = posture_names,\n",
        "    target_size=(width, height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='sparse',  \n",
        "    color_mode=\"rgb\"\n",
        ")\n",
        "\n",
        "validation_data = val_test_data_gen.flow_from_directory(\n",
        "    VALIDATION_DIR,\n",
        "    classes = posture_names,\n",
        "    target_size=(width, height),\n",
        "    batch_size=batch_size,\n",
        "    color_mode=\"rgb\",\n",
        "    class_mode='sparse'\n",
        ")\n",
        "\n",
        "test_data = val_test_data_gen.flow_from_directory(\n",
        "    TEST_DIR,\n",
        "    classes = posture_names,\n",
        "    target_size=(width, height),\n",
        "    batch_size=batch_size,\n",
        "    color_mode=\"rgb\",\n",
        "    class_mode='sparse'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Observemos una nuestro train data en lote "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Obtener un lote de imágenes y etiquetas del generador de datos\n",
        "images, labels = next(train_data)\n",
        "\n",
        "# Etiquetas de las clases (nombre de las carpetas)\n",
        "class_labels = list(train_data.class_indices.keys())\n",
        "print(class_labels)\n",
        "# Configurar subplots\n",
        "num_images = 8\n",
        "num_cols = 3 # Número de columnas en la cuadrícula\n",
        "num_rows = (num_images + num_cols - 1) // num_cols  # Calcular el número de filas necesarias\n",
        "\n",
        "# Crear una cuadrícula de subplots\n",
        "fig, axes = plt.subplots(num_rows, num_cols, figsize=(10, 5))\n",
        "\n",
        "# Iterar a través de las imágenes y subplots\n",
        "for i, ax in enumerate(axes.flatten()):\n",
        "    if i < num_images:\n",
        "        image = images[i]\n",
        "        label = class_labels[int(labels[i])]\n",
        "        \n",
        "        ax.imshow(image)\n",
        "        ax.set_title(f\"Clase: {label}\")\n",
        "        ax.axis('off')\n",
        "    else:\n",
        "        ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Defining our models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHpCmMa50sRY"
      },
      "outputs": [],
      "source": [
        "class Net(models.Model):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        # Primera serie de capas convolucionales y pooling\n",
        "        self.conv1_1 = layers.Conv2D(32, kernel_size=3, padding='same', activation='relu')\n",
        "        self.conv1_2 = layers.Conv2D(32, kernel_size=3, padding='same', activation='relu')\n",
        "        self.bn1 = layers.BatchNormalization()\n",
        "        self.pool1 = layers.MaxPooling2D(pool_size=(2, 2))\n",
        "\n",
        "        # Segunda serie de capas convolucionales y pooling\n",
        "        self.conv2_1 = layers.Conv2D(64, kernel_size=3, padding='same', activation='relu')\n",
        "        self.conv2_2 = layers.Conv2D(64, kernel_size=3, padding='same', activation='relu')\n",
        "        self.bn2 = layers.BatchNormalization()\n",
        "        self.pool2 = layers.MaxPooling2D(pool_size=(2, 2))\n",
        "\n",
        "        # Tercera serie de capas convolucionales y pooling\n",
        "        self.conv3_1 = layers.Conv2D(128, kernel_size=3, padding='same', activation='relu')\n",
        "        self.conv3_2 = layers.Conv2D(128, kernel_size=3, padding='same', activation='relu')\n",
        "        self.bn3 = layers.BatchNormalization()\n",
        "        self.pool3 = layers.MaxPooling2D(pool_size=(2, 2))\n",
        "\n",
        "        # Cuarta serie de capas convolucionales y pooling\n",
        "        self.conv4_1 = layers.Conv2D(256, kernel_size=3, padding='same', activation='relu')\n",
        "        self.conv4_2 = layers.Conv2D(256, kernel_size=3, padding='same', activation='relu')\n",
        "        self.bn4 = layers.BatchNormalization()\n",
        "        self.pool4 = layers.MaxPooling2D(pool_size=(2, 2))\n",
        "\n",
        "        self.flatten = layers.Flatten()\n",
        "\n",
        "        # Capas densas\n",
        "        self.fc1 = layers.Dense(512, activation='relu')\n",
        "        self.dropout1 = layers.Dropout(0.3)\n",
        "        self.fc2 = layers.Dense(256, activation='relu')\n",
        "        self.dropout2 = layers.Dropout(0.3)\n",
        "        self.fc3 = layers.Dense(128, activation='relu')\n",
        "        self.dropout3 = layers.Dropout(0.3)\n",
        "        self.fc4 = layers.Dense(4, activation='softmax')\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.pool1(self.bn1(self.conv1_2(self.conv1_1(x))))\n",
        "        x = self.pool2(self.bn2(self.conv2_2(self.conv2_1(x))))\n",
        "        x = self.pool3(self.bn3(self.conv3_2(self.conv3_1(x))))\n",
        "        x = self.pool4(self.bn4(self.conv4_2(self.conv4_1(x))))\n",
        "        x = self.flatten(x)\n",
        "        x = self.dropout1(self.fc1(x))\n",
        "        x = self.dropout2(self.fc2(x))\n",
        "        x = self.dropout3(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ImprovedNet(models.Model):\n",
        "    def __init__(self):\n",
        "        super(ImprovedNet, self).__init__()\n",
        "\n",
        "        # Primera serie de capas convolucionales y pooling\n",
        "        self.conv1_1 = layers.Conv2D(32, kernel_size=3, padding='same', activation='relu')\n",
        "        self.conv1_2 = layers.Conv2D(32, kernel_size=3, padding='same', activation='relu')\n",
        "        self.dropout1 = layers.Dropout(0.2)\n",
        "        self.pool1 = layers.MaxPooling2D(pool_size=(2, 2))\n",
        "\n",
        "        # Segunda serie de capas convolucionales y pooling\n",
        "        self.conv2_1 = layers.Conv2D(64, kernel_size=3, padding='same', activation='relu')\n",
        "        self.conv2_2 = layers.Conv2D(64, kernel_size=3, padding='same', activation='relu')\n",
        "        self.dropout2 = layers.Dropout(0.2)\n",
        "        self.pool2 = layers.MaxPooling2D(pool_size=(2, 2))\n",
        "\n",
        "        # Tercera serie de capas convolucionales y pooling\n",
        "        self.conv3_1 = layers.Conv2D(128, kernel_size=3, padding='same', activation='relu')\n",
        "        self.conv3_2 = layers.Conv2D(128, kernel_size=3, padding='same', activation='relu')\n",
        "        self.dropout3 = layers.Dropout(0.2)\n",
        "        self.pool3 = layers.MaxPooling2D(pool_size=(2, 2))\n",
        "\n",
        "        # Cuarta serie de capas convolucionales y pooling\n",
        "        self.conv4_1 = layers.Conv2D(256, kernel_size=3, padding='same', activation='relu')\n",
        "        self.conv4_2 = layers.Conv2D(256, kernel_size=3, padding='same', activation='relu')\n",
        "        self.dropout4 = layers.Dropout(0.2)\n",
        "        self.pool4 = layers.MaxPooling2D(pool_size=(2, 2))\n",
        "\n",
        "        # Quinta serie de capas convolucionales y pooling\n",
        "        self.conv5_1 = layers.Conv2D(512, kernel_size=3, padding='same', activation='relu')\n",
        "        self.conv5_2 = layers.Conv2D(512, kernel_size=3, padding='same', activation='relu')\n",
        "        self.dropout5 = layers.Dropout(0.2)\n",
        "        self.pool5 = layers.MaxPooling2D(pool_size=(2, 2))\n",
        "\n",
        "        self.flatten = layers.Flatten()\n",
        "\n",
        "        # Capas densas\n",
        "        self.fc1 = layers.Dense(64, activation='relu')\n",
        "        self.fc_dropout1 = layers.Dropout(0.3)\n",
        "        self.fc2 = layers.Dense(32, activation='relu')\n",
        "        self.fc_dropout2 = layers.Dropout(0.3)\n",
        "        self.fc3 = layers.Dense(4, activation='softmax')\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.pool1(self.dropout1(self.conv1_2(self.conv1_1(x))))\n",
        "        x = self.pool2(self.dropout2(self.conv2_2(self.conv2_1(x))))\n",
        "        x = self.pool3(self.dropout3(self.conv3_2(self.conv3_1(x))))\n",
        "        x = self.pool4(self.dropout4(self.conv4_2(self.conv4_1(x))))\n",
        "        x = self.pool5(self.dropout5(self.conv5_2(self.conv5_1(x))))\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc_dropout1(self.fc1(x))\n",
        "        x = self.fc_dropout2(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#using transfernet\n",
        "class TransferNet(models.Model):\n",
        "    def __init__(self, base_model_weights=None):\n",
        "        super(TransferNet, self).__init__()\n",
        "        self.base_model = tf.keras.applications.VGG16(include_top=False, weights=base_model_weights, input_shape=(224, 224, 3))\n",
        "        self.flatten = layers.Flatten()\n",
        "        # Capas densas\n",
        "        self.fc1 = layers.Dense(64, activation='relu')\n",
        "        self.fc_dropout1 = layers.Dropout(0.3)\n",
        "        self.fc2 = layers.Dense(32, activation='relu')\n",
        "        self.fc_dropout2 = layers.Dropout(0.3)\n",
        "        self.fc3 = layers.Dense(4, activation='softmax')\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.base_model(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc_dropout1(self.fc1(x))\n",
        "        x = self.fc_dropout2(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "    def get_config(self):\n",
        "        return {\"base_model_name\": \"VGG16\"}\n",
        "\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        if config[\"base_model_name\"] == \"VGG16\":\n",
        "            return cls(base_model_weights=\"imagenet\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ModifiedTransferNetVGG(models.Model):\n",
        "    def __init__(self, base_model_weights=None):\n",
        "        super(ModifiedTransferNetVGG, self).__init__()\n",
        "        self.base_model = tf.keras.applications.VGG16(include_top=False, weights=base_model_weights, input_shape=(224, 224, 3))\n",
        "        self.flatten = layers.Flatten()\n",
        "        \n",
        "        # Capa convolucional adicional\n",
        "        self.extra_conv = layers.Conv2D(512, kernel_size=3, padding='same', activation='relu')\n",
        "        self.extra_pool = layers.MaxPooling2D(pool_size=(2, 2))\n",
        "        self.extra_dropout = layers.Dropout(0.3)\n",
        "        \n",
        "        # Capas densas\n",
        "        self.fc1 = layers.Dense(128, activation='relu')\n",
        "        self.fc_dropout1 = layers.Dropout(0.4)\n",
        "        self.fc2 = layers.Dense(64, activation='relu')\n",
        "        self.fc_dropout2 = layers.Dropout(0.4)\n",
        "        self.fc3 = layers.Dense(32, activation='relu')\n",
        "        self.fc_dropout3 = layers.Dropout(0.4)\n",
        "        self.fc4 = layers.Dense(4, activation='softmax')\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.base_model(x)\n",
        "        x = self.extra_dropout(self.extra_pool(self.extra_conv(x)))\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc_dropout1(self.fc1(x))\n",
        "        x = self.fc_dropout2(self.fc2(x))\n",
        "        x = self.fc_dropout3(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ModifiedTransferNetResNet(models.Model):\n",
        "    def __init__(self, base_model_weights=None):\n",
        "        super(ModifiedTransferNetResNet, self).__init__()\n",
        "        self.base_model = tf.keras.applications.ResNet50(include_top=False, weights=base_model_weights, input_shape=(224, 224, 3))\n",
        "        self.flatten = layers.Flatten()\n",
        "        \n",
        "        # Capas densas\n",
        "        self.fc1 = layers.Dense(4096, activation='relu')\n",
        "        self.fc_dropout1 = layers.Dropout(0.2)\n",
        "        self.fc2 = layers.Dense(1024, activation='relu')\n",
        "        self.fc_dropout2 = layers.Dropout(0.3)\n",
        "        self.fc3 = layers.Dense(64, activation='relu')\n",
        "        self.fc_dropout3 = layers.Dropout(0.4)\n",
        "        self.fc4 = layers.Dense(4, activation='softmax')\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.base_model(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc_dropout1(self.fc1(x))\n",
        "        x = self.fc_dropout2(self.fc2(x))\n",
        "        x = self.fc_dropout3(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ImprovedTransferNetVGG19(models.Model):\n",
        "    def __init__(self, base_model_weights=None):\n",
        "        super(ImprovedTransferNetVGG19, self).__init__()\n",
        "        self.base_model = tf.keras.applications.VGG19(include_top=False, weights=base_model_weights, input_shape=(224, 224, 3))\n",
        "        for layer in self.base_model.layers:\n",
        "            layer.trainable = False  # Freeze VGG19 layers initially\n",
        "        self.flatten = layers.Flatten()\n",
        "        \n",
        "        l1lambda = 0.0001\n",
        "        # Dense layers with reduced neurons\n",
        "        self.fc1 = layers.Dense(1024, activation='relu',kernel_regularizer=tf.keras.regularizers.l2(l1lambda))\n",
        "        self.fc1_bn = layers.BatchNormalization()\n",
        "        self.fc1_dropout = layers.Dropout(0.4)\n",
        "\n",
        "        self.fc2 = layers.Dense(512, activation='relu')\n",
        "        self.fc2_bn = layers.BatchNormalization()\n",
        "        self.fc2_dropout = layers.Dropout(0.4)\n",
        "        \n",
        "        self.fc3 = layers.Dense(128, activation='relu')\n",
        "        self.fc3_bn = layers.BatchNormalization()\n",
        "        self.fc3_dropout = layers.Dropout(0.4)\n",
        "\n",
        "        self.fc4 = layers.Dense(4, activation='softmax')\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.base_model(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc1_bn(x)\n",
        "        x = self.fc1_dropout(x)\n",
        "        \n",
        "        x = self.fc2(x)\n",
        "        x = self.fc2_bn(x)\n",
        "        x = self.fc2_dropout(x)\n",
        "\n",
        "        x = self.fc3(x)\n",
        "        x = self.fc3_bn(x)\n",
        "        x = self.fc3_dropout(x)\n",
        "\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "    def unfreeze_last_layers(self, num_layers=6):\n",
        "        \"\"\"Desbloquea las últimas num_layers del modelo base.\"\"\"\n",
        "        for layer in self.base_model.layers[-num_layers:]:\n",
        "            layer.trainable = True\n",
        "\n",
        "    def get_config(self):\n",
        "        return {\"base_model_name\": \"VGG19\"}\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        if config[\"base_model_name\"] == \"VGG19\":\n",
        "            return cls(base_model_weights=\"imagenet\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TransferNetResNet50(models.Model):\n",
        "    def __init__(self, base_model_weights=None):\n",
        "        super(TransferNetResNet50, self).__init__()\n",
        "        self.base_model = tf.keras.applications.ResNet50(include_top=False, weights=base_model_weights, input_shape=(224, 224, 3))\n",
        "        for layer in self.base_model.layers:\n",
        "            layer.trainable = False  # Freeze ResNet50 layers initially\n",
        "        self.flatten = layers.Flatten()\n",
        "        \n",
        "        # Simplified dense layers\n",
        "        self.fc1 = layers.Dense(512, activation='relu')  # Reducing the number of neurons\n",
        "        self.fc_dropout1 = layers.Dropout(0.5)           # Increased dropout for better regularization\n",
        "        self.fc2 = layers.Dense(4, activation='softmax')\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.base_model(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc_dropout1(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "    def unfreeze_last_layers(self, num_layers=6):\n",
        "        \"\"\"Desbloquea las últimas num_layers del modelo base.\"\"\"\n",
        "        for layer in self.base_model.layers[-num_layers:]:\n",
        "            layer.trainable = True\n",
        "\n",
        "    def get_config(self):\n",
        "        return {\"base_model_name\": \"ResNet50\"}\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        if config[\"base_model_name\"] == \"ResNet50\":\n",
        "            return cls(base_model_weights=\"imagenet\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#model = Net()\n",
        "#model = ImprovedNet()\n",
        "#model = TransferNet() #esta es la 16\n",
        "#model = TransferNetVGG19()\n",
        "#model = TransferNetResNet50()\n",
        "model= ImprovedTransferNetVGG19()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "criterion = SparseCategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.00011887582580825929)\n",
        "EPOCHS=200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#learning rate schedulling\n",
        "def scheduler(epoch, lr):\n",
        "    if epoch < 10:\n",
        "        return lr\n",
        "    else:\n",
        "        return lr * tf.math.exp(-0.1)\n",
        "lr_schedule = tf.keras.callbacks.LearningRateScheduler(scheduler)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.compile(optimizer=optimizer, \n",
        "              loss=SparseCategoricalCrossentropy(), \n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "# Instancia tu modelo\n",
        "model_instance = ImprovedTransferNetVGG19(base_model_weights='imagenet')\n",
        "model_instance.build((None, 224, 224, 3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_model(model_instance, to_file='model.png', show_shapes=True, show_layer_names=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, verbose=0, restore_best_weights=True)\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint('checkpoint', monitor='val_loss', verbose=0, save_best_only=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "STEP_SIZE_TRAIN=train_data.n//train_data.batch_size\n",
        "STEP_SIZE_VALID=validation_data.n//validation_data.batch_size\n",
        "STEP_SIZE_TEST=test_data.n//test_data.batch_size\n",
        "STEP_SIZE_TRAIN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "callbacks = [early_stopping, checkpoint,lr_schedule]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "callbacks = [early_stopping, checkpoint,lr_schedule, tensorboard_callback]\n",
        "history =model.fit(train_data,\n",
        "    validation_data=validation_data,\n",
        "    epochs=30,\n",
        "    callbacks=callbacks\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loss_train = history.history['loss']\n",
        "loss_val = history.history['val_loss']\n",
        "epochs_loss_train = range(1,len(loss_train)+1)\n",
        "epochs_loss_val = range(1,len(loss_val)+1)\n",
        "plt.plot(epochs_loss_train, loss_train, 'g', label='Training loss')\n",
        "plt.plot(epochs_loss_val, loss_val, 'b', label='validation loss')\n",
        "plt.title('Training and Validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.savefig('training.png')\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "loss_train = history.history['accuracy']\n",
        "loss_val = history.history['val_accuracy']\n",
        "epochs_loss_train = range(1,len(loss_train)+1)\n",
        "epochs_loss_val = range(1,len(loss_val)+1)\n",
        "plt.plot(epochs_loss_train, loss_train, 'g', label='Training accuracy')\n",
        "plt.plot(epochs_loss_val, loss_val, 'b', label='validation accuracy')\n",
        "plt.title('Training and Validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.savefig('accuracy.png')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras.utils import plot_model\n",
        "plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.save_weights('yoga_vgg19_nofine.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Resultados del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Obtener las predicciones del modelo\n",
        "predictions = model.predict(test_data)\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Obtener las etiquetas verdaderas\n",
        "true_classes = test_data.classes\n",
        "class_labels = list(test_data.class_indices.keys())\n",
        "\n",
        "# Calcular la matriz de confusión\n",
        "confusion = confusion_matrix(true_classes, predicted_classes)\n",
        "\n",
        "# Visualizar la matriz de confusión\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(confusion, annot=True, cmap='Blues', fmt='g', xticklabels=class_labels, yticklabels=class_labels)\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Predice las etiquetas de tus datos de prueba.\n",
        "predictions = model.predict(test_data)\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "# 2. Obtén las etiquetas verdaderas de tus datos de prueba.\n",
        "true_classes = test_data.classes\n",
        "class_labels = list(test_data.class_indices.keys())\n",
        "\n",
        "# 3. Usa `classification_report` para obtener el informe.\n",
        "report = classification_report(true_classes, predicted_classes, target_names=class_labels,zero_division=1)\n",
        "print(report)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fine tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.unfreeze_last_layers(num_layers=6)\n",
        "\n",
        "# Compilar el modelo con un learning rate muy bajo\n",
        "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=1e-5)  # Usa una tasa de aprendizaje más baja\n",
        "model.compile(optimizer=optimizer, \n",
        "              loss=SparseCategoricalCrossentropy(), \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Continuar el entrenamiento\n",
        "history =model.fit(train_data,\n",
        "        steps_per_epoch=STEP_SIZE_TRAIN,\n",
        "        validation_data=validation_data,\n",
        "        validation_steps=STEP_SIZE_VALID,\n",
        "        epochs=30,\n",
        "        callbacks=callbacks\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loss_train = history.history['loss']\n",
        "loss_val = history.history['val_loss']\n",
        "epochs = range(1,20+1)\n",
        "plt.plot(epochs, loss_train, 'g', label='Training loss')\n",
        "plt.plot(epochs, loss_val, 'b', label='validation loss')\n",
        "plt.title('Training and Validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "    \n",
        "loss_train = history.history['accuracy']\n",
        "loss_val = history.history['val_accuracy']\n",
        "epochs = range(1,20+1)\n",
        "plt.plot(epochs, loss_train, 'g', label='Training accuracy')\n",
        "plt.plot(epochs, loss_val, 'b', label='validation accuracy')\n",
        "plt.title('Training and Validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Predice las etiquetas de tus datos de prueba.\n",
        "predictions = model.predict(test_data)\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "# 2. Obtén las etiquetas verdaderas de tus datos de prueba.\n",
        "true_classes = test_data.classes\n",
        "class_labels = list(test_data.class_indices.keys())\n",
        "\n",
        "# 3. Usa `classification_report` para obtener el informe.\n",
        "report = classification_report(true_classes, predicted_classes, target_names=class_labels,zero_division=1)\n",
        "print(report)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.save_weights('v5.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Searching best HiperParameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from kerastuner import HyperModel\n",
        "\n",
        "class TransferNetTuner(HyperModel):\n",
        "    def __init__(self, base_model_weights=None):\n",
        "        self.base_model_weights = base_model_weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_model(hp):\n",
        "        model = ImprovedTransferNetVGG19(base_model_weights='imagenet')\n",
        "        \n",
        "       # Hiperparámetro para el learning rate\n",
        "        lr = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG', default=1e-3)\n",
        "\n",
        "        # Hiperparámetro para el optimizador\n",
        "        optimizer = hp.Choice('optimizer', ['adam', 'sgd', 'rmsprop'])\n",
        "\n",
        "        # Compilación del modelo\n",
        "        if optimizer == 'adam':\n",
        "                opt = tf.keras.optimizers.legacy.Adam(learning_rate=lr)\n",
        "        elif optimizer == 'sgd':\n",
        "                opt = tf.keras.optimizers.SGD(learning_rate=lr)\n",
        "        elif optimizer == 'rmsprop':\n",
        "                opt = tf.keras.optimizers.RMSprop(learning_rate=lr)\n",
        "\n",
        "        model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "        return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',       # Métrica a monitorizar. En este caso, es la pérdida de validación.\n",
        "    patience=10,              # Número de épocas sin mejora después de las cuales se detiene el entrenamiento.\n",
        "    verbose=1,                # Para imprimir mensajes.\n",
        "    restore_best_weights=True # Restaura los pesos del modelo al mejor punto encontrado durante el entrenamiento.\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from kerastuner.tuners import RandomSearch\n",
        "half_steps_per_epoch = int(STEP_SIZE_TRAIN / 2)\n",
        "\n",
        "tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=5,\n",
        "    executions_per_trial=3,\n",
        "    directory='random_search_dir',\n",
        "    project_name='project_name_here'\n",
        ")\n",
        "\n",
        "tuner.search(train_data,\n",
        "             validation_data=validation_data,\n",
        "            steps_per_epoch=half_steps_per_epoch,\n",
        "             epochs=40,\n",
        "             callbacks=[early_stopping])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_hyperparameters = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "print(best_hyperparameters.values)\n",
        "#{'learning_rate': 0.001230253117597585 'optimizer': 'adam'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.summary()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
