{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSUD3mDAujuj"
      },
      "source": [
        "# Recognition of Yoga Postures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTnV1lGbu1OG"
      },
      "source": [
        "## Necessary libraries\n",
        "We choose\n",
        "[Pytorch](https://pytorch.org/)\n",
        "as our framework for this project for its learning parh and It is Simple GPU installation.\n",
        "And we are going to work with"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wdFyomexudoh"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/johanjairgilcesreyes/miniconda3/envs/myenv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "import random\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers, models\n",
        "from shutil import copyfile\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from utils import split_data\n",
        "from fastai.vision.all import get_image_files,verify_images\n",
        "import gradio as gr\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "from tensorflow.keras.losses import categorical_crossentropy,SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.optimizers.legacy import Adam\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# check if CUDA is available to use GPU on the training \n",
        "train_on_gpu = tf.test.is_gpu_available()\n",
        "\n",
        "if train_on_gpu:\n",
        "    print('GPU con soporte CUDA disponible')\n",
        "else:\n",
        "    print('No se encontró GPU con soporte CUDA')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "images=get_image_files('../YOGA/dataset')\n",
        "images\n",
        "failed = verify_images(images)\n",
        "failed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "images=get_image_files('../YOGA/DATA')\n",
        "images\n",
        "failed = verify_images(images)\n",
        "failed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "images=get_image_files('../YOGA/dataset/test/')\n",
        "images\n",
        "failed = verify_images(images)\n",
        "failed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pezi0oUO0U54"
      },
      "outputs": [],
      "source": [
        "# Define paths\n",
        "downdog_SRC_DIR = \"data/downdog\"\n",
        "goddess_SRC_DIR = \"data/goddess\"\n",
        "plank_SRC_DIR = \"data/plank\"\n",
        "tree_SRC_DIR = \"data/tree\"\n",
        "warrior_SRC_DIR = \"data/warrior\"\n",
        "TRAINING_DIR = \"dataset/training\"\n",
        "VALIDATION_DIR = \"dataset/validation\"\n",
        "TEST_DIR = \"dataset/test\"\n",
        "#splitting the dataset\n",
        "TRAINING_DDG_DIR = os.path.join(TRAINING_DIR, \"downdog/\")\n",
        "VALIDATION_DDG_DIR = os.path.join(VALIDATION_DIR, \"downdog/\")\n",
        "TEST_DDG_DIR = os.path.join(TEST_DIR, \"downdog/\")\n",
        "\n",
        "TRAINING_GDS_DIR = os.path.join(TRAINING_DIR, \"goddess/\")\n",
        "VALIDATION_GDS_DIR = os.path.join(VALIDATION_DIR, \"goddess/\")\n",
        "TEST_GDS_DIR = os.path.join(TEST_DIR, \"goddess/\")\n",
        "\n",
        "TRAINING_PLANK_DIR = os.path.join(TRAINING_DIR, \"plank/\")\n",
        "VALIDATION_PLANK_DIR = os.path.join(VALIDATION_DIR, \"plank/\")\n",
        "TEST_PLANK_DIR = os.path.join(TEST_DIR, \"plank/\")\n",
        "\n",
        "TRAINING_TREE_DIR = os.path.join(TRAINING_DIR, \"tree/\")\n",
        "VALIDATION_TREE_DIR = os.path.join(VALIDATION_DIR, \"tree/\")\n",
        "TEST_TREE_DIR = os.path.join(TEST_DIR, \"tree/\")\n",
        "\n",
        "TRAINING_WR_DIR = os.path.join(TRAINING_DIR, \"warrior/\")\n",
        "VALIDATION_WR_DIR = os.path.join(VALIDATION_DIR, \"warrior/\")\n",
        "TEST_WR_DIR = os.path.join(TEST_DIR, \"warrior/\")\n",
        "# Define proportion of images used for training\n",
        "train_size = 0.8\n",
        "val_size = 0.1\n",
        "test_size = 0.1\n",
        "\n",
        "split_data(downdog_SRC_DIR,\n",
        "           TRAINING_DDG_DIR,\n",
        "           VALIDATION_DDG_DIR,\n",
        "           TEST_DDG_DIR,\n",
        "           split_ratio=(train_size,val_size, test_size))\n",
        "\n",
        "split_data(goddess_SRC_DIR,\n",
        "           TRAINING_GDS_DIR,\n",
        "           VALIDATION_GDS_DIR,\n",
        "           TEST_GDS_DIR,\n",
        "           split_ratio=(train_size,val_size, test_size))\n",
        "\n",
        "split_data(plank_SRC_DIR,\n",
        "           TRAINING_PLANK_DIR,\n",
        "           VALIDATION_PLANK_DIR,\n",
        "           TEST_PLANK_DIR,\n",
        "           split_ratio=(train_size,val_size, test_size))\n",
        "\n",
        "split_data(tree_SRC_DIR,\n",
        "           TRAINING_TREE_DIR,\n",
        "           VALIDATION_TREE_DIR,\n",
        "           TEST_TREE_DIR,\n",
        "           split_ratio=(train_size,val_size, test_size))\n",
        "split_data(warrior_SRC_DIR,\n",
        "           TRAINING_WR_DIR,\n",
        "           VALIDATION_WR_DIR,\n",
        "           TEST_WR_DIR,\n",
        "           split_ratio=(train_size,val_size, test_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Original Downdog directory has {len(os.listdir(downdog_SRC_DIR))} images\")\n",
        "print(f\"Original Goddess directory has {len(os.listdir(goddess_SRC_DIR))} images\")\n",
        "print(f\"Original Plank directory has {len(os.listdir(plank_SRC_DIR))} images\")\n",
        "print(f\"Original Tree directory has {len(os.listdir(tree_SRC_DIR))} images\")\n",
        "print(f\"Original Warrior directory has {len(os.listdir(warrior_SRC_DIR))} images\\n\")\n",
        "\n",
        "# Training and validation splits\n",
        "print(f\"There are {len(os.listdir(TRAINING_DDG_DIR))} images of Dogdown for training\")\n",
        "print(f\"There are {len(os.listdir(TRAINING_GDS_DIR))} images of Goddess for training\")\n",
        "print(f\"There are {len(os.listdir(TRAINING_PLANK_DIR))} images of Plank for training\")\n",
        "print(f\"There are {len(os.listdir(TRAINING_TREE_DIR))} images of Tree for training\")\n",
        "print(f\"There are {len(os.listdir(TRAINING_WR_DIR))} images of Warrior for training\\n\")\n",
        "\n",
        "print(f\"There are {len(os.listdir(VALIDATION_DDG_DIR))} images of Dogdown for validation\")           \n",
        "print(f\"There are {len(os.listdir(VALIDATION_GDS_DIR))} images of Goddess for validation\")\n",
        "print(f\"There are {len(os.listdir(VALIDATION_PLANK_DIR))} images of Plank for validation\")\n",
        "print(f\"There are {len(os.listdir(VALIDATION_TREE_DIR))} images of Tree for validation\")\n",
        "print(f\"There are {len(os.listdir(VALIDATION_WR_DIR))} images of Warrior for validation\\n\")\n",
        "\n",
        "print(f\"There are {len(os.listdir(TEST_DDG_DIR))} images of Dogdown for test\")\n",
        "print(f\"There are {len(os.listdir(TEST_GDS_DIR))} images of Goddess for test\")\n",
        "print(f\"There are {len(os.listdir(TEST_PLANK_DIR))} images of Plank for test\")\n",
        "print(f\"There are {len(os.listdir(TEST_TREE_DIR))} images of Plank for test\")\n",
        "print(f\"There are {len(os.listdir(TEST_WR_DIR))} images of tree for test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_euw7YFhx58o"
      },
      "source": [
        "## Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOq3rJ-l0Okp"
      },
      "outputs": [],
      "source": [
        "# Definir un generador de imágenes con transformaciones\n",
        "batch_size = 16\n",
        "width=250\n",
        "height=250\n",
        "data_generator = ImageDataGenerator(\n",
        "    rescale=1.0/255,\n",
        "    rotation_range=10,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.3,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest',\n",
        "    validation_split=0.20\n",
        ")\n",
        "\n",
        "\n",
        "# Cargar imágenes del directorio y aplicar transformaciones\n",
        "train_data = data_generator.flow_from_directory(\n",
        "    TRAINING_DIR,\n",
        "    target_size=(width, height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='sparse',  \n",
        "    color_mode=\"rgb\"\n",
        ")\n",
        "\n",
        "validation_data = data_generator.flow_from_directory(\n",
        "    VALIDATION_DIR,\n",
        "    target_size=(width, height),\n",
        "    batch_size=batch_size,\n",
        "    color_mode=\"rgb\",\n",
        "    class_mode='sparse',\n",
        ")\n",
        "\n",
        "test_data = data_generator.flow_from_directory(\n",
        "    TEST_DIR,\n",
        "    target_size=(width, height),\n",
        "    batch_size=batch_size,\n",
        "    color_mode=\"rgb\",\n",
        "    class_mode='sparse',\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for data, labels in train_data:\n",
        "    print(labels)\n",
        "    # Aquí puedes agregar más código para inspeccionar los datos y etiquetas\n",
        "    break  # Detenemos el ciclo después de imprimir las etiquetas de un lote\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Observemos una imagen "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Obtener un lote de imágenes y etiquetas del generador de datos\n",
        "images, labels = next(train_data)\n",
        "\n",
        "# Etiquetas de las clases (nombre de las carpetas)\n",
        "class_labels = list(train_data.class_indices.keys())\n",
        "print(class_labels)\n",
        "# Configurar subplots\n",
        "num_images = 8\n",
        "num_cols = 3 # Número de columnas en la cuadrícula\n",
        "num_rows = (num_images + num_cols - 1) // num_cols  # Calcular el número de filas necesarias\n",
        "\n",
        "# Crear una cuadrícula de subplots\n",
        "fig, axes = plt.subplots(num_rows, num_cols, figsize=(10, 5))\n",
        "\n",
        "# Iterar a través de las imágenes y subplots\n",
        "for i, ax in enumerate(axes.flatten()):\n",
        "    if i < num_images:\n",
        "        image = images[i]\n",
        "        label = class_labels[int(labels[i])]\n",
        "        \n",
        "        ax.imshow(image)\n",
        "        ax.set_title(f\"Clase: {label}\")\n",
        "        ax.axis('off')\n",
        "    else:\n",
        "        ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHpCmMa50sRY"
      },
      "outputs": [],
      "source": [
        "class Net(models.Model):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        # Primera serie de capas convolucionales y pooling\n",
        "        self.conv1_1 = layers.Conv2D(32, kernel_size=3, padding='same', activation='relu')\n",
        "        self.conv1_2 = layers.Conv2D(32, kernel_size=3, padding='same', activation='relu')\n",
        "        self.bn1 = layers.BatchNormalization()\n",
        "        self.pool1 = layers.MaxPooling2D(pool_size=(2, 2))\n",
        "\n",
        "        # Segunda serie de capas convolucionales y pooling\n",
        "        self.conv2_1 = layers.Conv2D(64, kernel_size=3, padding='same', activation='relu')\n",
        "        self.conv2_2 = layers.Conv2D(64, kernel_size=3, padding='same', activation='relu')\n",
        "        self.bn2 = layers.BatchNormalization()\n",
        "        self.pool2 = layers.MaxPooling2D(pool_size=(2, 2))\n",
        "\n",
        "        # Tercera serie de capas convolucionales y pooling\n",
        "        self.conv3_1 = layers.Conv2D(128, kernel_size=3, padding='same', activation='relu')\n",
        "        self.conv3_2 = layers.Conv2D(128, kernel_size=3, padding='same', activation='relu')\n",
        "        self.bn3 = layers.BatchNormalization()\n",
        "        self.pool3 = layers.MaxPooling2D(pool_size=(2, 2))\n",
        "\n",
        "        # Cuarta serie de capas convolucionales y pooling\n",
        "        self.conv4_1 = layers.Conv2D(256, kernel_size=3, padding='same', activation='relu')\n",
        "        self.conv4_2 = layers.Conv2D(256, kernel_size=3, padding='same', activation='relu')\n",
        "        self.bn4 = layers.BatchNormalization()\n",
        "        self.pool4 = layers.MaxPooling2D(pool_size=(2, 2))\n",
        "\n",
        "        self.flatten = layers.Flatten()\n",
        "\n",
        "        # Capas densas\n",
        "        self.fc1 = layers.Dense(512, activation='relu')\n",
        "        self.dropout1 = layers.Dropout(0.3)\n",
        "        self.fc2 = layers.Dense(256, activation='relu')\n",
        "        self.dropout2 = layers.Dropout(0.3)\n",
        "        self.fc3 = layers.Dense(128, activation='relu')\n",
        "        self.dropout3 = layers.Dropout(0.3)\n",
        "        self.fc4 = layers.Dense(5, activation='softmax')\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.pool1(self.bn1(self.conv1_2(self.conv1_1(x))))\n",
        "        x = self.pool2(self.bn2(self.conv2_2(self.conv2_1(x))))\n",
        "        x = self.pool3(self.bn3(self.conv3_2(self.conv3_1(x))))\n",
        "        x = self.pool4(self.bn4(self.conv4_2(self.conv4_1(x))))\n",
        "        x = self.flatten(x)\n",
        "        x = self.dropout1(self.fc1(x))\n",
        "        x = self.dropout2(self.fc2(x))\n",
        "        x = self.dropout3(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ImprovedNet(models.Model):\n",
        "    def __init__(self):\n",
        "        super(ImprovedNet, self).__init__()\n",
        "\n",
        "        # Primera serie de capas convolucionales y pooling\n",
        "        self.conv1_1 = layers.Conv2D(32, kernel_size=3, padding='same', activation='relu')\n",
        "        self.conv1_2 = layers.Conv2D(32, kernel_size=3, padding='same', activation='relu')\n",
        "        self.dropout1 = layers.Dropout(0.2)\n",
        "        self.pool1 = layers.MaxPooling2D(pool_size=(2, 2))\n",
        "\n",
        "        # Segunda serie de capas convolucionales y pooling\n",
        "        self.conv2_1 = layers.Conv2D(64, kernel_size=3, padding='same', activation='relu')\n",
        "        self.conv2_2 = layers.Conv2D(64, kernel_size=3, padding='same', activation='relu')\n",
        "        self.dropout2 = layers.Dropout(0.2)\n",
        "        self.pool2 = layers.MaxPooling2D(pool_size=(2, 2))\n",
        "\n",
        "        # Tercera serie de capas convolucionales y pooling\n",
        "        self.conv3_1 = layers.Conv2D(128, kernel_size=3, padding='same', activation='relu')\n",
        "        self.conv3_2 = layers.Conv2D(128, kernel_size=3, padding='same', activation='relu')\n",
        "        self.dropout3 = layers.Dropout(0.2)\n",
        "        self.pool3 = layers.MaxPooling2D(pool_size=(2, 2))\n",
        "\n",
        "        # Cuarta serie de capas convolucionales y pooling\n",
        "        self.conv4_1 = layers.Conv2D(256, kernel_size=3, padding='same', activation='relu')\n",
        "        self.conv4_2 = layers.Conv2D(256, kernel_size=3, padding='same', activation='relu')\n",
        "        self.dropout4 = layers.Dropout(0.2)\n",
        "        self.pool4 = layers.MaxPooling2D(pool_size=(2, 2))\n",
        "\n",
        "        # Quinta serie de capas convolucionales y pooling\n",
        "        self.conv5_1 = layers.Conv2D(512, kernel_size=3, padding='same', activation='relu')\n",
        "        self.conv5_2 = layers.Conv2D(512, kernel_size=3, padding='same', activation='relu')\n",
        "        self.dropout5 = layers.Dropout(0.2)\n",
        "        self.pool5 = layers.MaxPooling2D(pool_size=(2, 2))\n",
        "\n",
        "        self.flatten = layers.Flatten()\n",
        "\n",
        "        # Capas densas\n",
        "        self.fc1 = layers.Dense(64, activation='relu')\n",
        "        self.fc_dropout1 = layers.Dropout(0.3)\n",
        "        self.fc2 = layers.Dense(32, activation='relu')\n",
        "        self.fc_dropout2 = layers.Dropout(0.3)\n",
        "        self.fc3 = layers.Dense(5, activation='softmax')\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.pool1(self.dropout1(self.conv1_2(self.conv1_1(x))))\n",
        "        x = self.pool2(self.dropout2(self.conv2_2(self.conv2_1(x))))\n",
        "        x = self.pool3(self.dropout3(self.conv3_2(self.conv3_1(x))))\n",
        "        x = self.pool4(self.dropout4(self.conv4_2(self.conv4_1(x))))\n",
        "        x = self.pool5(self.dropout5(self.conv5_2(self.conv5_1(x))))\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc_dropout1(self.fc1(x))\n",
        "        x = self.fc_dropout2(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class TransferNet(models.Model):\n",
        "    def __init__(self, base_model_weights=None):\n",
        "        super(TransferNet, self).__init__()\n",
        "        self.base_model = tf.keras.applications.VGG16(include_top=False, weights=base_model_weights, input_shape=(250, 250, 3))\n",
        "        self.flatten = layers.Flatten()\n",
        "        # Capas densas\n",
        "        self.fc1 = layers.Dense(64, activation='relu')\n",
        "        self.fc_dropout1 = layers.Dropout(0.3)\n",
        "        self.fc2 = layers.Dense(32, activation='relu')\n",
        "        self.fc_dropout2 = layers.Dropout(0.3)\n",
        "        self.fc3 = layers.Dense(5, activation='softmax')\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.base_model(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc_dropout1(self.fc1(x))\n",
        "        x = self.fc_dropout2(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "    def get_config(self):\n",
        "        return {\"base_model_name\": \"VGG16\"}\n",
        "\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        if config[\"base_model_name\"] == \"VGG16\":\n",
        "            return cls(base_model_weights=\"imagenet\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ModifiedTransferNetVGG(models.Model):\n",
        "    def __init__(self, base_model_weights=None):\n",
        "        super(ModifiedTransferNetVGG, self).__init__()\n",
        "        self.base_model = tf.keras.applications.VGG16(include_top=False, weights=base_model_weights, input_shape=(250, 250, 3))\n",
        "        self.flatten = layers.Flatten()\n",
        "        \n",
        "        # Capa convolucional adicional\n",
        "        self.extra_conv = layers.Conv2D(512, kernel_size=3, padding='same', activation='relu')\n",
        "        self.extra_pool = layers.MaxPooling2D(pool_size=(2, 2))\n",
        "        self.extra_dropout = layers.Dropout(0.3)\n",
        "        \n",
        "        # Capas densas\n",
        "        self.fc1 = layers.Dense(128, activation='relu')\n",
        "        self.fc_dropout1 = layers.Dropout(0.4)\n",
        "        self.fc2 = layers.Dense(64, activation='relu')\n",
        "        self.fc_dropout2 = layers.Dropout(0.4)\n",
        "        self.fc3 = layers.Dense(32, activation='relu')\n",
        "        self.fc_dropout3 = layers.Dropout(0.4)\n",
        "        self.fc4 = layers.Dense(5, activation='softmax')\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.base_model(x)\n",
        "        x = self.extra_dropout(self.extra_pool(self.extra_conv(x)))\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc_dropout1(self.fc1(x))\n",
        "        x = self.fc_dropout2(self.fc2(x))\n",
        "        x = self.fc_dropout3(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ModifiedTransferNetResNet(models.Model):\n",
        "    def __init__(self, base_model_weights=None):\n",
        "        super(ModifiedTransferNetResNet, self).__init__()\n",
        "        self.base_model = tf.keras.applications.ResNet50(include_top=False, weights=base_model_weights, input_shape=(250, 250, 3))\n",
        "        self.flatten = layers.Flatten()\n",
        "        \n",
        "        # Capa convolucional adicional\n",
        "        self.extra_conv = layers.Conv2D(512, kernel_size=3, padding='same', activation='relu')\n",
        "        self.extra_pool = layers.MaxPooling2D(pool_size=(2, 2))\n",
        "        self.extra_dropout = layers.Dropout(0.3)\n",
        "        \n",
        "        # Capas densas\n",
        "        self.fc1 = layers.Dense(128, activation='relu')\n",
        "        self.fc_dropout1 = layers.Dropout(0.4)\n",
        "        self.fc2 = layers.Dense(64, activation='relu')\n",
        "        self.fc_dropout2 = layers.Dropout(0.4)\n",
        "        self.fc3 = layers.Dense(32, activation='relu')\n",
        "        self.fc_dropout3 = layers.Dropout(0.4)\n",
        "        self.fc4 = layers.Dense(5, activation='softmax')\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.base_model(x)\n",
        "        x = self.extra_dropout(self.extra_pool(self.extra_conv(x)))\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc_dropout1(self.fc1(x))\n",
        "        x = self.fc_dropout2(self.fc2(x))\n",
        "        x = self.fc_dropout3(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#model = Net()\n",
        "#model = ImprovedNet()\n",
        "model = TransferNet()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "criterion = SparseCategoricalCrossentropy()\n",
        "optimizer = Adam(learning_rate=0.0004230253)\n",
        "#optimizer = tf.keras.optimizers.legacy.RMSprop(learning_rate=0.001)\n",
        "\n",
        "EPOCHS=20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(len(train_data),len(validation_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def scheduler(epoch, lr):\n",
        "    if epoch < 10:\n",
        "        return lr\n",
        "    else:\n",
        "        return lr * tf.math.exp(-0.1)\n",
        "lr_schedule = tf.keras.callbacks.LearningRateScheduler(scheduler)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.compile(optimizer=optimizer, \n",
        "              loss=SparseCategoricalCrossentropy(), \n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, verbose=0, restore_best_weights=True)\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint('checkpoint', monitor='val_loss', verbose=0, save_best_only=True)\n",
        "#callbacks = [early_stopping, checkpoint, lr_schedule]\n",
        "callbacks = [early_stopping, checkpoint]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "STEP_SIZE_TRAIN=train_data.n//train_data.batch_size\n",
        "STEP_SIZE_VALID=validation_data.n//validation_data.batch_size\n",
        "STEP_SIZE_TEST=test_data.n//test_data.batch_size\n",
        "STEP_SIZE_TRAIN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "callbacks = [early_stopping, checkpoint, tensorboard_callback]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history =model.fit(train_data,\n",
        "    step_per_epoch=STEP_SIZE_TRAIN,\n",
        "    validation_data=validation_data,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=callbacks\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loss_train = history.history['loss']\n",
        "loss_val = history.history['val_loss']\n",
        "epochs = range(1,EPOCHS+1)\n",
        "plt.plot(epochs, loss_train, 'g', label='Training loss')\n",
        "plt.plot(epochs, loss_val, 'b', label='validation loss')\n",
        "plt.title('Training and Validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "    \n",
        "loss_train = history.history['accuracy']\n",
        "loss_val = history.history['val_accuracy']\n",
        "epochs = range(1,EPOCHS+1)\n",
        "plt.plot(epochs, loss_train, 'g', label='Training accuracy')\n",
        "plt.plot(epochs, loss_val, 'b', label='validation accuracy')\n",
        "plt.title('Training and Validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.save_weights('yoga_weights.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#for gradio\n",
        "def predict_image(img):\n",
        "    # Redimensionar y preprocesar la imagen\n",
        "    img = img.reshape(1, width, height, 3)\n",
        "    img = img / 255.0\n",
        "\n",
        "    # Hacer la predicción\n",
        "    predictions = model.predict(img)\n",
        "    predicted_class = np.argmax(predictions)\n",
        "\n",
        "    # Obtener el nombre de la clase\n",
        "    class_names = list(train_data.class_indices.keys())\n",
        "    return class_names[predicted_class]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "interface = gr.Interface(\n",
        "    fn=predict_image, \n",
        "    inputs=gr.inputs.Image(shape=(width, height)), \n",
        "    outputs=\"text\",\n",
        "    live=True,\n",
        "    capture_session=True\n",
        ")\n",
        "interface.launch()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Resultados del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Obtener las predicciones del modelo\n",
        "predictions = model.predict(test_data)\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Obtener las etiquetas verdaderas\n",
        "true_classes = test_data.classes\n",
        "class_labels = list(test_data.class_indices.keys())\n",
        "\n",
        "# Calcular la matriz de confusión\n",
        "confusion = confusion_matrix(true_classes, predicted_classes)\n",
        "\n",
        "# Visualizar la matriz de confusión\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(confusion, annot=True, cmap='Blues', fmt='g', xticklabels=class_labels, yticklabels=class_labels)\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "\n",
        "# 1. Predice las etiquetas de tus datos de prueba.\n",
        "predictions = model.predict(test_data)\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "# 2. Obtén las etiquetas verdaderas de tus datos de prueba.\n",
        "true_classes = test_data.classes\n",
        "class_labels = list(test_data.class_indices.keys())\n",
        "\n",
        "# 3. Usa `classification_report` para obtener el informe.\n",
        "report = classification_report(true_classes, predicted_classes, target_names=class_labels)\n",
        "print(report)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Asegúrate de que el modelo ha sido llamado al menos una vez\n",
        "dummy_input = np.random.random((1, width, height, 3))\n",
        "_ = model(dummy_input)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fine tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for layer in base_model.layers[-5:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# Compilar el modelo con un learning rate muy bajo\n",
        "optimizer_fine_tuning = tf.keras.optimizers.Adam(learning_rate=0.00001)\n",
        "model.compile(optimizer=optimizer_fine_tuning, \n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(), \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Continuar el entrenamiento\n",
        "history =model.fit(train_data,\n",
        "        steps_per_epoch=STEP_SIZE_TRAIN,\n",
        "        validation_data=validation_data,\n",
        "        validation_steps=STEP_SIZE_VALID,\n",
        "        epochs=EPOCHS,\n",
        "        callbacks=callbacks\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loss_train = history.history['loss']\n",
        "loss_val = history.history['val_loss']\n",
        "epochs = range(1,EPOCHS+1)\n",
        "plt.plot(epochs, loss_train, 'g', label='Training loss')\n",
        "plt.plot(epochs, loss_val, 'b', label='validation loss')\n",
        "plt.title('Training and Validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "    \n",
        "loss_train = history.history['accuracy']\n",
        "loss_val = history.history['val_accuracy']\n",
        "epochs = range(1,EPOCHS+1)\n",
        "plt.plot(epochs, loss_train, 'g', label='Training accuracy')\n",
        "plt.plot(epochs, loss_val, 'b', label='validation accuracy')\n",
        "plt.title('Training and Validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Searching best HiperParameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from kerastuner import HyperModel\n",
        "\n",
        "class TransferNetTuner(HyperModel):\n",
        "    def __init__(self, base_model_weights=None):\n",
        "        self.base_model_weights = base_model_weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_model(hp):\n",
        "        model = TransferNet(base_model_weights='imagenet')\n",
        "        \n",
        "       # Hiperparámetro para el learning rate\n",
        "        lr = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG', default=1e-3)\n",
        "\n",
        "        # Hiperparámetro para el optimizador\n",
        "        optimizer = hp.Choice('optimizer', ['adam', 'sgd', 'rmsprop'])\n",
        "\n",
        "        # Compilación del modelo\n",
        "        if optimizer == 'adam':\n",
        "                opt = tf.keras.optimizers.legacy.Adam(learning_rate=lr)\n",
        "        elif optimizer == 'sgd':\n",
        "                opt = tf.keras.optimizers.SGD(learning_rate=lr)\n",
        "        elif optimizer == 'rmsprop':\n",
        "                opt = tf.keras.optimizers.RMSprop(learning_rate=lr)\n",
        "\n",
        "        model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "        return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',       # Métrica a monitorizar. En este caso, es la pérdida de validación.\n",
        "    patience=10,              # Número de épocas sin mejora después de las cuales se detiene el entrenamiento.\n",
        "    verbose=1,                # Para imprimir mensajes.\n",
        "    restore_best_weights=True # Restaura los pesos del modelo al mejor punto encontrado durante el entrenamiento.\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from kerastuner.tuners import RandomSearch\n",
        "\n",
        "# Usar solo la mitad de los datos para acelerar la búsqueda\n",
        "\n",
        "half_steps_per_epoch = int(STEP_SIZE_TRAIN / 2)\n",
        "\n",
        "tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=5,\n",
        "    executions_per_trial=3,\n",
        "    directory='random_search_dir',\n",
        "    project_name='project_name_here'\n",
        ")\n",
        "\n",
        "\n",
        "tuner.search(train_data,\n",
        "             validation_data=validation_data,\n",
        "            steps_per_epoch=half_steps_per_epoch,\n",
        "             epochs=5,\n",
        "             callbacks=[early_stopping])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_hyperparameters = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "print(best_hyperparameters.values)\n",
        "#{'learning_rate': 0.004230253117597585}"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
